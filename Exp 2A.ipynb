{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP 2A Exercises.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SUy4sNh1yOF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text=\"God is Great! I won a lottery.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMRgCkOs5D4F",
        "colab_type": "text"
      },
      "source": [
        "Tokenize Word\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y90zVISe3oRv",
        "colab_type": "code",
        "outputId": "bf28bb76-a42b-43e5-d297-8e6a7e280c0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dh66-Z3E4A1s",
        "colab_type": "code",
        "outputId": "4a6db8aa-b362-4792-8b07-6501b208016d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "print(word_tokenize(text))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['God', 'is', 'Great', '!', 'I', 'won', 'a', 'lottery', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoZH_LSk5KoN",
        "colab_type": "text"
      },
      "source": [
        "Tokenize Sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqTOaT4q4I98",
        "colab_type": "code",
        "outputId": "9393d4fd-2fb2-4135-8e24-fad50b6861c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "print(sent_tokenize(text))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['God is Great!', 'I won a lottery.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJgEGF-u8fZF",
        "colab_type": "text"
      },
      "source": [
        "Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nKMLtVc4v8U",
        "colab_type": "code",
        "outputId": "2330843e-9475-41a2-9fb2-cdea96f6fb26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "e_words= [\"wait\", \"waiting\", \"waited\", \"waits\"]\n",
        "ps =PorterStemmer()\n",
        "for w in e_words:\n",
        "  rootWord=ps.stem(w)\n",
        "print(rootWord)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wait\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVhKmqZYABF7",
        "colab_type": "code",
        "outputId": "51e08e03-0d1d-498b-a0fd-1d8d43e5d375",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "sentence=sentence=\"Hello Guru99, You have to build a very good site and I love visiting your site.\"\n",
        "words = word_tokenize(sentence)\n",
        "ps = PorterStemmer()\n",
        "for w in words:\n",
        "  rootWord=ps.stem(w)\n",
        "  print(rootWord)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hello\n",
            "guru99\n",
            ",\n",
            "you\n",
            "have\n",
            "to\n",
            "build\n",
            "a\n",
            "veri\n",
            "good\n",
            "site\n",
            "and\n",
            "I\n",
            "love\n",
            "visit\n",
            "your\n",
            "site\n",
            ".\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2SKEN2PEe_R",
        "colab_type": "code",
        "outputId": "e9f42fd5-552b-46a0-ddc7-d784c1238755",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "porter_stemmer = PorterStemmer()\n",
        "text = \"studies studying cries cry\"\n",
        "tokenization = nltk.word_tokenize(text)\n",
        "for w in tokenization:\n",
        "  print(\"Stemming for {} is {}\".format(w,porter_stemmer.stem(w)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stemming for studies is studi\n",
            "Stemming for studying is studi\n",
            "Stemming for cries is cri\n",
            "Stemming for cry is cri\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEpv-FKrKsUm",
        "colab_type": "text"
      },
      "source": [
        "Lemmatization code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bx3YSu61LCX8",
        "colab_type": "code",
        "outputId": "77f2f686-e23a-4d8c-e77c-26c059b88f0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29rIi4ulFpIq",
        "colab_type": "code",
        "outputId": "aa814fad-30b3-4900-9fe1-4f598b2c0a95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "text = \"studies studying cries cry\"\n",
        "tokenization = nltk.word_tokenize(text)\n",
        "for w in tokenization:\n",
        "  print(\"Lemma for {} is {}\".format(w,wordnet_lemmatizer.lemmatize(w)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lemma for studies is study\n",
            "Lemma for studying is studying\n",
            "Lemma for cries is cry\n",
            "Lemma for cry is cry\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rg0-91tSLQJs",
        "colab_type": "text"
      },
      "source": [
        "Removal of Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xx8m7xGaLjsU",
        "colab_type": "code",
        "outputId": "aad3eb1c-d779-4dd9-e60d-7a400870c99e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9KZlpvMKwAK",
        "colab_type": "code",
        "outputId": "06b06f56-a73c-47ef-e007-bec396f2bd79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
        "stop_words = set(stopwords.words('english'))\n",
        "word_tokens = word_tokenize(example_sent)\n",
        "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
        "filtered_sentence = []\n",
        "for w in word_tokens:\n",
        "  if w not in stop_words:\n",
        "    filtered_sentence.append(w)\n",
        "print(word_tokens)\n",
        "print(filtered_sentence)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
            "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5StrEGyVe5CS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}